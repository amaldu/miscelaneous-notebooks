# Project Walkthrough

Here I explain the step-by-step logic and workflow followed in this machine learning project, from data collection to model evaluation. 

The goal of the project is to predict Uber ride cancellations using machine learning to enable proactive intervention by creating strategies to reduce the overall cancellation rate.

# Problem Framing
## 1.1 Defininition of the objective in business terms

Looking at the Uber ride data from 2024, I identified a critical operational challenge, 37.430 rides out of 148.770 the total bookings never reached completion. 

This means that the 25% (37.430 rides) of all bookings end in cancellation of which a 19.15% (27.000 rides) are made by customers and 7.45% (10.500 rides) are made by drivers. This as a result, means that for every 4 ride requests, 1 fails to complete. 

In terms of business impact, I can think of the following areas:

1. Financial impact:
   1. Revenue loss: the dataset does not provide the ride prices but from my experience, if I estimate an averange booking value of 20$ per ride without taking into consideration the type of vehicle, the 37.430 cancelled rides represent 748.600$ in lost revenue in 2024. 
   2. Hidden operational costs: there are other costs like the time and fuel wasted by drivers while the ride is not cancelled yet, the costs of the processing platform (servers, computations, payment platform usage) and the potential churn of those frustrated customers.
   3. Opportunity cost: every cancellation activates resources within the company that also have a cost, some examples can be contacting customer supports, making refunds, managing complains, etc. They cost money that could be invested in other areas to promote the growth of the company.

2. Operational impact:
   1. Imbalance in supply and demand: when drivers are removed from the pool of available services while they are on their way to pick up the customer. If the price per ride depends on the amount of available drivers, this can artificially increase the price of the available services and potentially damaging the image of the company. 
   2. Driver insatisfaction: drivers are also consuming Uber's service and they also become frustrated and potentially change their providers if the customer is cancelling their rides too often. 
   
Let's imagine that Uber wants to reduce the cancellation rate by 10%, this means 3.700 more rides completed and almost 75000$ recovered so building a predictive model that identifies bookings with high cancellation risk at the time of booking could help achieving it.

## 1.2 How will the solution be used?

The model will be deployed as a real-time prediction system integrated into Uber's booking workflow so when a customer requests a ride, the model will score the cancellation probability.

This will help other parts of the company to develop and implement long-term fixes like:

- Increase customer engagement in high-risk rides: send confirmation messages, increase ETA updates, create loyalty points, etc. 
- Cluster and route drivers: based on their experience in completion and ratings, Uber can send drivers to higher risk rides.

All those techniques can also be monitored accross time and regions to better understand issues in their implementation and evolution over time.

## 1.3 What are the current solutions/workarounds (if any)?

Neither the metadata nor the dataset provide any other information about different approaches previously taken by the company.

## 1.4 How should the problem be framed

I'm going to start framing it as a supervised binary classification problem: cancelled vs. completed even though in future approaches it would be interesting to try a multi- class approach: cancelled by customer / driver vs. completed vs. incompleted if business value is demonstrated. 

The training will be offline on historical batch data I would plan to re-train it daily and monitor concept drift to check if concept drift requires faster adaptation and we have to change to near-online or online training.

The inference will be in real time when booking happens. 

## 1.5 How should performance be measured?

The dataset shows that we have a cancellation rate of 25%, which is moderately imbalanced.

Given the business context, I need metrics that balance catching cancellations (recall) with not overwhelming operations with false alarms (precision) but let's define events before: 

1. Event : Cancellation
   The customer receives incentive, the company has the reassign the driver wherever he is, 
Before training any model, I define minimum acceptable recall and precision based on business cost and operational constraints.
**Primary Metrics**:

1. Recall (Sensitivity) % of actual cancellations correctly predicted | ≥ 70% |
| **Precision** | % of predicted cancellations that were actual cancellations | ≥ 60% |
| **F1-Score** | Harmonic mean of precision and recall | ≥ 0.65 |
| **AUC-ROC** | Model's ability to distinguish between classes | ≥ 0.75 |

**Why Recall is Prioritized**:
- Missing a cancellation (False Negative) means lost revenue and poor customer experience
- A false alarm (False Positive) only costs a proactive message or incentive
- The cost of intervention is lower than the cost of cancellation

**Secondary Metrics**:
- **Precision-Recall AUC**: Better for imbalanced datasets
- **Confusion Matrix**: Detailed breakdown of prediction types
- **Lift Chart**: Business value of model vs. random selection

**Business KPIs to Track**:
- Overall cancellation rate reduction
- Revenue recovered from prevented cancellations
- Customer satisfaction scores
- Driver utilization improvement
- 
- We model business cost assuming that missing a cancellation (FN) is 5x more costly than triggering an unnecessary intervention (FP). This assumption was validated with product and ops teams and tested via sensitivity analysis.

## 1.6 Is the performance measure aligned with the business objective?

**Yes, with the following alignment**:

| Business Objective | Performance Measure | Alignment |
|-------------------|---------------------|-----------|
| Reduce cancellation rate | High Recall | Catching more at-risk bookings enables intervention |
| Minimize operational overhead | Reasonable Precision | Avoid flooding ops team with false alarms |
| Improve customer experience | F1-Score balance | Balanced approach prevents over-intervention |
| Maximize ROI | AUC-ROC | Overall model quality ensures sustainable improvement |

**Cost-Benefit Analysis**:

- **Cost of False Negative** (missed cancellation): Lost booking value (~$15-50 average) + customer dissatisfaction
- **Cost of False Positive** (unnecessary intervention): Cost of incentive (~$2-5) + operational time
- **Ratio**: ~10:1 favoring recall over precision

This justifies our emphasis on recall while maintaining acceptable precision.

### 1.7 What would be the minimum performance needed to reach the business objective?

**Minimum Viable Performance**:

| Metric | Minimum Threshold | Rationale |
|--------|-------------------|-----------|
| Recall | 60% | Must catch majority of cancellations to be useful |
| Precision | 50% | At least half of flagged bookings should be true risks |
| F1-Score | 0.55 | Baseline for balanced performance |
| AUC-ROC | 0.70 | Must be significantly better than random (0.50) |

**Break-Even Analysis**:

If intervention cost is $3 per flagged booking and average saved booking value is $25:
- Break-even precision = $3 / $25 = 12%
- Any precision above 12% generates positive ROI
- Our 50% minimum threshold provides significant safety margin

**Stretch Goals**:

| Metric | Target | Stretch |
|--------|--------|---------|
| Recall | 70% | 80% |
| Precision | 60% | 70% |
| F1-Score | 0.65 | 0.75 |
| AUC-ROC | 0.75 | 0.85 |

### 1.8 What are comparable problems? Can you reuse experience or tools?

**Directly Comparable Problems**:

1. **Customer Churn Prediction**
   - Similar: Predicting customer behavior before it happens
   - Reusable: Feature engineering approaches, imbalanced classification techniques
   - Tools: Scikit-learn, XGBoost, SHAP for interpretability

2. **Hotel/Restaurant No-Show Prediction**
   - Similar: Predicting booking cancellations in service industry
   - Reusable: Time-based features, customer history patterns
   - Research: Extensive literature on hospitality no-show prediction

3. **Flight Cancellation Prediction**
   - Similar: Transportation booking cancellations
   - Reusable: Weather, time, and route-based features
   - Difference: More external factors (weather) in flights

4. **E-commerce Cart Abandonment**
   - Similar: Predicting incomplete transactions
   - Reusable: Behavioral features, session analysis
   - Tools: Funnel analysis techniques

**Reusable Tools and Techniques**:

| Tool/Technique | Application |
|----------------|-------------|
| XGBoost/LightGBM | High-performance gradient boosting |
| SMOTE/ADASYN | Handling class imbalance |
| SHAP | Model interpretability |
| Optuna/Hyperopt | Hyperparameter optimization |
| MLflow | Experiment tracking |
| Scikit-learn Pipelines | Reproducible preprocessing |

### 1.9 Is human expertise available?

**Available Expertise**:

1. **Domain Knowledge from Dataset**:
   - Cancellation reasons are documented (Wrong Address, Driver Issues, etc.)
   - Vehicle type performance metrics available
   - Payment method patterns documented

2. **Industry Knowledge**:
   - Ride-sharing operations best practices
   - Common cancellation triggers in transportation
   - Customer behavior patterns in on-demand services

3. **Data Science Expertise**:
   - Classification modeling experience
   - Imbalanced data handling
   - Feature engineering for temporal data

**Knowledge to Acquire**:
- Specific Uber operational constraints
- Regional factors affecting NCR (National Capital Region) rides
- Peak hour definitions and surge pricing impact

### 1.10 How would you solve the problem manually?

**Manual Rule-Based Approach**:

A human analyst would flag bookings as high-risk based on:

1. **Time-Based Rules**:
   - Late night bookings (11 PM - 5 AM): Higher cancellation risk
   - Rush hour bookings: Driver availability issues
   - Weekend vs. weekday patterns

2. **Location-Based Rules**:
   - Known problematic pickup locations (poor GPS, restricted access)
   - Long-distance routes: Higher driver cancellation
   - Airport/station pickups: Customer plan changes

3. **Vehicle Type Rules**:
   - Premium vehicles (Premier Sedan): Lower cancellation
   - Budget options (Auto, eBike): Customer price sensitivity

4. **Customer History** (if available):
   - Previous cancellation history
   - Rating patterns
   - Payment method reliability

5. **Real-Time Factors**:
   - High VTAT (Vehicle Time to Arrival): Customer impatience
   - Surge pricing active: Customer may cancel after seeing final price

**Limitations of Manual Approach**:
- Cannot process thousands of bookings in real-time
- Rules are static, don't adapt to changing patterns
- Cannot capture complex feature interactions
- Inconsistent application across different operators

### 1.11 List the assumptions you (or others) have made so far

**Data Assumptions**:

1. **Representativeness**: The 2024 dataset is representative of typical booking patterns
2. **Data Quality**: Cancellation reasons are accurately recorded
3. **Completeness**: All relevant features are captured in the dataset
4. **Stationarity**: Patterns in 2024 will persist into future periods
5. **No Data Leakage**: Features available at booking time don't include post-booking information

**Business Assumptions**:

6. **Intervention Effectiveness**: Proactive measures can actually prevent cancellations
7. **Cost Structure**: Intervention costs are lower than cancellation costs
8. **Operational Capacity**: Operations team can act on model predictions
9. **Customer Response**: Customers will respond positively to interventions

**Technical Assumptions**:

10. **Feature Availability**: All features used in training will be available at inference time
11. **Latency Requirements**: Model can score bookings within acceptable time (<100ms)
12. **Infrastructure**: Deployment infrastructure exists or can be built

**Model Assumptions**:

13. **Linear Separability**: Some degree of separability exists between cancelled and completed rides
14. **Feature Relevance**: Available features contain signal for prediction
15. **Generalization**: Model trained on historical data will generalize to new bookings

### 1.12 Verify assumptions if possible

**Verifiable Assumptions**:

| Assumption | Verification Method | Status |
|------------|---------------------|--------|
| Data completeness | Check missing value rates | To verify in EDA |
| Feature distributions | Statistical analysis | To verify in EDA |
| Class balance | Count target classes | Verified: 25% cancellation rate |
| Temporal patterns | Time series analysis | To verify in EDA |
| Feature correlations | Correlation analysis | To verify in EDA |

**Assumptions Requiring Business Input**:

| Assumption | Required Information |
|------------|---------------------|
| Intervention effectiveness | A/B test results or pilot data |
| Cost structure | Finance team input |
| Operational capacity | Operations team assessment |

**Assumptions to Monitor Post-Deployment**:

| Assumption | Monitoring Approach |
|------------|---------------------|
| Pattern stationarity | Model performance drift detection |
| Feature availability | Data pipeline monitoring |
| Customer response | Intervention success rate tracking |

---

## 2. Get the Data

### 2.1 List the data you need and how much you need

**Required Data**:

| Data Type | Description | Minimum Size |
|-----------|-------------|--------------|
| Historical Bookings | Complete booking records with outcomes | 100,000+ records |
| Booking Features | Time, location, vehicle type, payment method | All available columns |
| Outcome Labels | Booking status (completed, cancelled, incomplete) | For all records |
| Cancellation Details | Reason codes for cancelled rides | Where available |
| Customer/Driver IDs | For behavioral pattern analysis | Anonymized identifiers |

**Available Data**:
- **148,770 booking records** from 2024
- **20 columns** covering all required feature categories
- **Full year coverage** with daily granularity
- **Cancellation reasons** for both customer and driver cancellations

**Data Sufficiency Assessment**:
- ✅ Sample size adequate for ML modeling (>100K records)
- ✅ Positive class (cancellations) has sufficient examples (~37K)
- ✅ Multiple feature categories available
- ✅ Temporal coverage spans full year (seasonal patterns captured)

### 2.2 Find and document where you can get that data

**Data Source**:

| Attribute | Value |
|-----------|-------|
| Platform | Kaggle |
| Dataset Name | Uber Ride Analytics Dashboard |
| Dataset Reference | `yashdevladdha/uber-ride-analytics-dashboard` |
| URL | https://www.kaggle.com/datasets/yashdevladdha/uber-ride-analytics-dashboard |
| File Format | CSV |
| Primary File | `ncr_ride_bookings.csv` |

**Acquisition Method**:
```python
from kaggle.api.kaggle_api_extended import KaggleApi

api = KaggleApi()
api.authenticate()

dataset_ref = 'yashdevladdha/uber-ride-analytics-dashboard'
api.dataset_download_files(dataset_ref, path='../data/raw', unzip=True)
```

**Additional Files in Dataset**:
- `Uber.pbix` - Power BI dashboard file
- `Dashboard.gif` - Visualization preview

### 2.3 Check how much space it will take

**Storage Requirements**:

| Item | Size (Estimated) |
|------|------------------|
| Raw CSV file | ~15-25 MB |
| Processed datasets | ~30-50 MB |
| Model artifacts | ~5-10 MB |
| Notebooks & outputs | ~20-30 MB |
| **Total Workspace** | **~100 MB** |

**Memory Requirements**:
- DataFrame in memory: ~50-100 MB (with all columns loaded)
- Training with full dataset: ~500 MB - 1 GB RAM
- Sufficient for local development on standard hardware

### 2.4 Check legal obligations, and get authorization if necessary

**Legal Assessment**:

| Consideration | Status |
|---------------|--------|
| Data License | Public Kaggle dataset - free to use for analysis |
| Personal Data | Customer/Driver IDs are anonymized identifiers |
| Commercial Use | Check Kaggle dataset license terms |
| GDPR/Privacy | No personally identifiable information (PII) |
| Attribution | Credit dataset creator in publications |

**Data Privacy Compliance**:
- ✅ No real names or contact information
- ✅ Location data is categorical (area names), not GPS coordinates
- ✅ Booking IDs are synthetic identifiers
- ✅ No sensitive financial details (only payment method type)

### 2.5 Get access authorizations

**Access Requirements**:

1. **Kaggle Account**: Required for API access
2. **API Credentials**: `kaggle.json` file with username and key
3. **Environment Setup**: 
   ```bash
   export KAGGLE_USERNAME="your_username"
   export KAGGLE_KEY="your_api_key"
   ```

**Authorization Status**: ✅ Data already downloaded (see `01_ingest_data.ipynb`)

### 2.6 Create a workspace (with enough storage space)

**Workspace Structure**:

```
uber-analysis/
├── data/
│   ├── raw/                    # Original unmodified data
│   │   └── ncr_ride_bookings.csv
│   ├── processed/              # Cleaned and transformed data
│   └── interim/                # Intermediate processing files
├── notebooks/
│   ├── 01_ingest_data.ipynb    # Data acquisition
│   ├── 02_eda.ipynb            # Exploratory data analysis
│   ├── 03_preprocessing.ipynb  # Data preparation
│   └── 04_modeling.ipynb       # Model training and evaluation
├── src/                        # Source code modules
│   ├── data/                   # Data loading utilities
│   ├── features/               # Feature engineering
│   └── models/                 # Model training code
├── models/                     # Saved model artifacts
├── reports/                    # Generated analysis reports
│   └── figures/                # Visualizations
├── DATASET_INFO.md             # Dataset documentation
└── PROJECT_WALKTHROUGH.md      # This file
```

**Storage Allocation**: Local workspace with ~1 GB available space (sufficient)

### 2.7 Get the data

**Data Acquisition Status**: ✅ Completed

The data has been downloaded using the Kaggle API as documented in `notebooks/01_ingest_data.ipynb`:

```python
dataset_ref = 'yashdevladdha/uber-ride-analytics-dashboard'
download_path = '../data/raw'
api.dataset_download_files(dataset_ref, path=download_path, unzip=True)
```

**Verification**:
- File location: `uber-analysis/data/raw/ncr_ride_bookings.csv`
- Download date: As per notebook execution
- Integrity: CSV file loads successfully with pandas

### 2.8 Convert the data to a format you can easily manipulate

**Data Format Strategy**:

| Stage | Format | Purpose |
|-------|--------|---------|
| Raw | CSV | Original data preservation |
| Working | Pandas DataFrame | Analysis and transformation |
| Processed | Parquet | Efficient storage with types preserved |
| Features | NumPy arrays | Model training input |

**Loading Code**:
```python
import pandas as pd

# Load raw data
df = pd.read_csv('../data/raw/ncr_ride_bookings.csv')

# Parse date/time columns
df['Date'] = pd.to_datetime(df['Date'])
df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.time

# Save processed version
df.to_parquet('../data/processed/bookings_clean.parquet')
```

**Type Conversions Needed**:
- `Date`: string → datetime
- `Time`: string → time object
- Categorical columns: object → category (memory optimization)
- Boolean flags: object → bool

### 2.9 Ensure sensitive information is deleted or protected

**Sensitive Data Assessment**:

| Column | Sensitivity | Action |
|--------|-------------|--------|
| Customer ID | Low (anonymized) | Keep as-is |
| Booking ID | Low (synthetic) | Keep as-is |
| Pickup Location | Low (area names) | Keep as-is |
| Drop Location | Low (area names) | Keep as-is |
| Payment Method | Low (type only) | Keep as-is |
| Booking Value | Medium | Aggregate for reporting |

**Privacy Measures**:
- ✅ No PII in dataset
- ✅ IDs are already anonymized
- ✅ No need for additional anonymization
- ⚠️ Avoid publishing individual booking details in reports

### 2.10 Check the size and type of data

**Dataset Characteristics**:

| Attribute | Value |
|-----------|-------|
| Total Records | 148,770 |
| Total Features | 20 columns |
| Data Type | Tabular (structured) |
| Temporal Scope | Year 2024 |
| Geographic Scope | NCR (National Capital Region) |
| Update Frequency | Static dataset |

**Data Types by Category**:

| Category | Columns | Type |
|----------|---------|------|
| Temporal | Date, Time | datetime |
| Identifiers | Booking ID, Customer ID | string/int |
| Categorical | Vehicle Type, Booking Status, Payment Method, Locations | category |
| Numerical | Avg VTAT, Avg CTAT, Booking Value, Ride Distance, Ratings | float |
| Boolean | Cancelled Rides by Customer, Cancelled Rides by Driver, Incomplete Rides | bool |
| Text | Cancellation reasons | string |

**Time Series Considerations**:
- Data has temporal ordering (Date, Time)
- Can analyze trends, seasonality, and patterns
- Train/test split should respect temporal order for realistic evaluation

### 2.11 Sample a test set, put it aside, and never look at it

**Test Set Strategy**:

**Approach**: Stratified temporal split to maintain:
1. Class distribution (cancellation rate)
2. Temporal ordering (avoid data leakage)

**Split Ratios**:

| Set | Percentage | Records | Purpose |
|-----|------------|---------|---------|
| Training | 70% | ~104,000 | Model training |
| Validation | 15% | ~22,000 | Hyperparameter tuning |
| Test | 15% | ~22,000 | Final evaluation (held out) |

**Implementation**:
```python
from sklearn.model_selection import train_test_split

# Create binary target
df['is_cancelled'] = df['Booking Status'].isin([
    'Cancelled by Customer', 
    'Cancelled by Driver'
]).astype(int)

# First split: separate test set
X_temp, X_test, y_temp, y_test = train_test_split(
    df.drop('is_cancelled', axis=1),
    df['is_cancelled'],
    test_size=0.15,
    stratify=df['is_cancelled'],
    random_state=42
)

# Second split: training and validation
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp,
    test_size=0.176,  # 0.176 of 0.85 ≈ 0.15 of total
    stratify=y_temp,
    random_state=42
)
```

**Test Set Rules**:
- ⛔ Never use test set for feature engineering decisions
- ⛔ Never use test set for hyperparameter tuning
- ⛔ Never look at test set performance until final model selection
- ✅ Only evaluate final model once on test set
- ✅ Report test set metrics as final performance estimate

**Alternative: Time-Based Split**:
For more realistic evaluation, consider chronological split:
```python
# Sort by date
df_sorted = df.sort_values('Date')

# Use last 15% of data as test set
split_idx = int(len(df_sorted) * 0.85)
train_val_df = df_sorted.iloc[:split_idx]
test_df = df_sorted.iloc[split_idx:]
```

---

## 3. Explore the Data

> **Note**: Try to get insights from a field expert for these steps.

### 3.1 Create a copy of the data for exploration

**Exploration Data Strategy**:

```python
import pandas as pd

# Load raw data
df_raw = pd.read_csv('../data/raw/ncr_ride_bookings.csv')

# Create exploration copy (never modify raw data)
df_explore = df_raw.copy()

# For large datasets, sample if necessary (not needed here - 148K is manageable)
# df_sample = df_explore.sample(n=50000, random_state=42)
```

**Data Preservation Rules**:
- ✅ Raw data in `data/raw/` remains untouched
- ✅ All exploration done on copies
- ✅ Transformations documented and reproducible

### 3.2 Create a Jupyter notebook to keep a record of your data exploration

**EDA Notebook Structure** (`notebooks/02_eda.ipynb`):

```
1. Setup and Data Loading
   - Import libraries
   - Load dataset
   - Initial inspection

2. Data Quality Assessment
   - Missing values analysis
   - Duplicate detection
   - Data type validation

3. Univariate Analysis
   - Target variable distribution
   - Numerical features distributions
   - Categorical features distributions

4. Bivariate Analysis
   - Features vs. target
   - Feature correlations
   - Cross-tabulations

5. Temporal Analysis
   - Trends over time
   - Seasonality patterns
   - Day/hour patterns

6. Geographic Analysis
   - Location-based patterns
   - Route analysis

7. Key Findings Summary
   - Insights documentation
   - Feature importance indicators
   - Recommendations for modeling
```

### 3.3 Study each attribute and its characteristics

**Complete Attribute Analysis**:

#### Temporal Attributes

| Column | Type | Description | Usefulness | Distribution |
|--------|------|-------------|------------|--------------|
| **Date** | datetime | Booking date | High - temporal patterns | Full year 2024, daily granularity |
| **Time** | time | Booking time | High - peak hour patterns | 24-hour coverage |

**Derived Features to Create**:
- Hour of day (0-23)
- Day of week (0-6)
- Is weekend (boolean)
- Is peak hour (boolean: 7-10 AM, 5-9 PM)
- Month
- Week of year

#### Identifier Attributes

| Column | Type | Description | Usefulness | Notes |
|--------|------|-------------|------------|-------|
| **Booking ID** | string | Unique booking identifier | None for modeling | Drop for ML |
| **Customer ID** | string | Customer identifier | Medium - repeat customer patterns | Aggregate for features |

**Customer-Level Features to Create**:
- Customer booking count (historical)
- Customer cancellation rate (historical)
- Customer average rating given

#### Target Variable

| Column | Type | Description | Distribution |
|--------|------|-------------|--------------|
| **Booking Status** | categorical | Outcome of booking | Completed: 65.96%, Customer Cancelled: 19.15%, Driver Cancelled: 7.45%, Other: 7.44% |

**Target Engineering**:
```python
# Binary target for classification
df['is_cancelled'] = df['Booking Status'].isin([
    'Cancelled by Customer',
    'Cancelled by Driver'
]).astype(int)

# Multi-class target (alternative)
df['cancellation_type'] = df['Booking Status'].map({
    'Completed': 0,
    'Cancelled by Customer': 1,
    'Cancelled by Driver': 2,
    'Incomplete': 3
})
```

#### Vehicle Attributes

| Column | Type | Description | Usefulness | Distribution |
|--------|------|-------------|------------|--------------|
| **Vehicle Type** | categorical | Type of vehicle booked | High - different cancellation rates | Auto (highest), eBike, Go Mini, Go Sedan, Premier Sedan, UberXL (lowest bookings) |

**Vehicle Type Analysis** (from DATASET_INFO):

| Vehicle Type | Total Bookings | Success Rate |
|--------------|----------------|--------------|
| Auto | 12.88M | 91.1% |
| eBike/Bike | 11.46M | 91.1% |
| Go Mini | 10.34M | 91.0% |
| Go Sedan | 9.37M | 91.1% |
| Premier Sedan | 6.28M | 91.2% |
| UberXL | 1.53M | 92.2% |

**Insight**: UberXL has highest success rate (92.2%) - premium vehicles may have lower cancellation.

#### Location Attributes

| Column | Type | Description | Usefulness | Notes |
|--------|------|-------------|------------|-------|
| **Pickup Location** | categorical | Starting location | High - location patterns | Area/neighborhood names |
| **Drop Location** | categorical | Destination | Medium - route analysis | Area/neighborhood names |

**Location Features to Create**:
- Is same area pickup/drop
- Route popularity (count of same route bookings)
- Known problematic locations flag

#### Time Duration Attributes

| Column | Type | Description | Usefulness | Distribution |
|--------|------|-------------|------------|--------------|
| **Avg VTAT** | float | Vehicle Time to Arrival (minutes) | High - wait time affects cancellation | Continuous, likely right-skewed |
| **Avg CTAT** | float | Customer Trip Time (minutes) | Medium - trip duration | Continuous |

**Hypothesis**: Higher VTAT (longer wait) → Higher cancellation probability

**Features to Create**:
- VTAT buckets (0-5, 5-10, 10-15, 15+ minutes)
- VTAT vs. vehicle type interaction
- Is long wait (VTAT > 10 minutes)

#### Cancellation Detail Attributes

| Column | Type | Description | Usefulness | Notes |
|--------|------|-------------|------------|-------|
| **Cancelled Rides by Customer** | boolean | Customer cancellation flag | Target component | ~19.15% of bookings |
| **Reason for cancelling by Customer** | text | Cancellation reason | Post-hoc analysis | Not available at prediction time |
| **Cancelled Rides by Driver** | boolean | Driver cancellation flag | Target component | ~7.45% of bookings |
| **Driver Cancellation Reason** | text | Driver's reason | Post-hoc analysis | Not available at prediction time |
| **Incomplete Rides** | boolean | Incomplete ride flag | Target consideration | Separate category |
| **Incomplete Rides Reason** | text | Incompletion reason | Post-hoc analysis | Not available at prediction time |

**Important Note**: Cancellation reasons are NOT available at prediction time (they occur after cancellation). These columns are useful for:
- Understanding cancellation patterns
- Post-hoc analysis
- NOT for feature engineering (data leakage)

**Customer Cancellation Reasons** (from DATASET_INFO):
- Wrong Address: 22.5%
- Driver Issues: 22.4%
- Driver Not Moving: 22.2%
- Change of Plans: 21.9%
- App Issues: 11.0%

**Driver Cancellation Reasons**:
- Capacity Issues: 25.0%
- Customer Related Issues: 25.3%
- Personal & Car Issues: 24.9%
- Customer Behavior: 24.8%

#### Financial Attributes

| Column | Type | Description | Usefulness | Distribution |
|--------|------|-------------|------------|--------------|
| **Booking Value** | float | Fare amount | Medium - price sensitivity | Continuous, currency |
| **Payment Method** | categorical | Payment type | Medium - payment behavior | UPI (~40%), Cash (~25%), Credit Card (~15%), Uber Wallet (~12%), Debit Card (~8%) |

**Payment Method Analysis**:
- UPI dominates (digital payment preference)
- Cash still significant (25%)
- Hypothesis: Cash payments may have different cancellation patterns

#### Distance Attribute

| Column | Type | Description | Usefulness | Distribution |
|--------|------|-------------|------------|--------------|
| **Ride Distance** | float | Trip distance (km) | Medium - long trips may have higher cancellation | Continuous, ~26 km average |

**Features to Create**:
- Distance buckets (short: <10km, medium: 10-30km, long: >30km)
- Distance per booking value (value density)

#### Rating Attributes

| Column | Type | Description | Usefulness | Distribution |
|--------|------|-------------|------------|--------------|
| **Driver Ratings** | float | Rating given to driver (1-5) | Low - post-ride data | Mean: 4.23-4.24 |
| **Customer Rating** | float | Rating given by customer (1-5) | Low - post-ride data | Mean: 4.40-4.41 |

**Important Note**: Ratings are collected AFTER the ride, so they cannot be used as features for predicting cancellation of the current ride. However, historical ratings could be aggregated:
- Average driver rating (from previous rides)
- Average customer rating (from previous rides)

### 3.4 For supervised learning tasks, identify the target attribute(s)

**Primary Target Variable**:

| Attribute | Column | Type | Classes |
|-----------|--------|------|---------|
| **Binary Classification** | `is_cancelled` | int | 0 (Completed), 1 (Cancelled) |

**Target Distribution**:
```
Completed:    93,000 (65.96%)  → Class 0
Cancelled:    37,430 (25.15%)  → Class 1
  - By Customer: 27,000 (19.15%)
  - By Driver:   10,500 (7.45%)
Incomplete:   ~18,340 (7.44%)  → Exclude or separate class
```

**Class Imbalance**:
- Ratio: ~2.5:1 (Completed : Cancelled)
- Moderate imbalance - manageable with standard techniques
- Techniques to consider: SMOTE, class weights, threshold tuning

**Alternative Target Formulations**:

1. **Multi-class Classification**:
   - Class 0: Completed
   - Class 1: Cancelled by Customer
   - Class 2: Cancelled by Driver
   - Benefit: Different interventions for different cancellation types

2. **Probability Regression**:
   - Output: Cancellation probability (0-1)
   - Benefit: Risk scoring, threshold flexibility

### 3.5 Visualize the data

**Visualization Plan**:

#### Target Variable Distribution
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Booking status distribution
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Pie chart
df['Booking Status'].value_counts().plot.pie(ax=axes[0], autopct='%1.1f%%')
axes[0].set_title('Booking Status Distribution')

# Bar chart
sns.countplot(data=df, x='Booking Status', ax=axes[1])
axes[1].set_title('Booking Counts by Status')
plt.tight_layout()
```

#### Temporal Patterns
```python
# Cancellation rate by hour
hourly_cancel = df.groupby(df['Time'].dt.hour)['is_cancelled'].mean()
hourly_cancel.plot(kind='line', marker='o')
plt.title('Cancellation Rate by Hour of Day')
plt.xlabel('Hour')
plt.ylabel('Cancellation Rate')

# Cancellation rate by day of week
daily_cancel = df.groupby(df['Date'].dt.dayofweek)['is_cancelled'].mean()
daily_cancel.plot(kind='bar')
plt.title('Cancellation Rate by Day of Week')
```

#### Vehicle Type Analysis
```python
# Cancellation rate by vehicle type
vehicle_cancel = df.groupby('Vehicle Type')['is_cancelled'].agg(['mean', 'count'])
vehicle_cancel.plot(kind='bar', y='mean')
plt.title('Cancellation Rate by Vehicle Type')
```

#### Numerical Features Distribution
```python
# Distribution of key numerical features
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

df['Avg VTAT'].hist(ax=axes[0,0], bins=50)
axes[0,0].set_title('Distribution of VTAT (Wait Time)')

df['Avg CTAT'].hist(ax=axes[0,1], bins=50)
axes[0,1].set_title('Distribution of CTAT (Trip Time)')

df['Booking Value'].hist(ax=axes[1,0], bins=50)
axes[1,0].set_title('Distribution of Booking Value')

df['Ride Distance'].hist(ax=axes[1,1], bins=50)
axes[1,1].set_title('Distribution of Ride Distance')
```

#### Correlation Heatmap
```python
# Correlation matrix for numerical features
numerical_cols = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance', 'is_cancelled']
corr_matrix = df[numerical_cols].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation Matrix')
```

### 3.6 Study the correlations between attributes

**Expected Correlations**:

| Feature Pair | Expected Correlation | Rationale |
|--------------|---------------------|-----------|
| Ride Distance ↔ Booking Value | Strong positive | Longer rides cost more |
| Ride Distance ↔ CTAT | Strong positive | Longer rides take more time |
| VTAT ↔ Cancellation | Moderate positive | Longer waits → more cancellations |
| Booking Value ↔ Cancellation | Weak negative | Premium rides may have lower cancellation |

**Correlation Analysis Code**:
```python
# Numerical correlations
print("Correlation with target (is_cancelled):")
print(df[numerical_cols].corr()['is_cancelled'].sort_values(ascending=False))

# Categorical correlations (using chi-square)
from scipy.stats import chi2_contingency

def cramers_v(x, y):
    """Calculate Cramer's V for categorical association"""
    contingency = pd.crosstab(x, y)
    chi2 = chi2_contingency(contingency)[0]
    n = contingency.sum().sum()
    min_dim = min(contingency.shape) - 1
    return np.sqrt(chi2 / (n * min_dim))

# Calculate for categorical features
categorical_cols = ['Vehicle Type', 'Payment Method', 'Pickup Location', 'Drop Location']
for col in categorical_cols:
    v = cramers_v(df[col], df['is_cancelled'])
    print(f"{col}: Cramer's V = {v:.4f}")
```

### 3.7 Study how you would solve the problem manually

**Manual Decision Rules** (baseline for model comparison):

```python
def manual_cancellation_prediction(row):
    """
    Rule-based cancellation prediction based on domain knowledge
    """
    risk_score = 0
    
    # Rule 1: High wait time increases risk
    if row['Avg VTAT'] > 15:
        risk_score += 3
    elif row['Avg VTAT'] > 10:
        risk_score += 2
    elif row['Avg VTAT'] > 7:
        risk_score += 1
    
    # Rule 2: Late night bookings
    hour = row['Time'].hour
    if hour >= 23 or hour <= 5:
        risk_score += 2
    
    # Rule 3: Budget vehicle types (hypothesis)
    if row['Vehicle Type'] in ['Auto', 'eBike/Bike']:
        risk_score += 1
    
    # Rule 4: Cash payments (hypothesis - may cancel if no cash)
    if row['Payment Method'] == 'Cash':
        risk_score += 1
    
    # Rule 5: Very long distance
    if row['Ride Distance'] > 40:
        risk_score += 1
    
    # Threshold for prediction
    return 1 if risk_score >= 4 else 0
```

**Baseline Performance Target**: The ML model should significantly outperform this rule-based approach.

### 3.8 Identify the promising transformations you may want to apply

**Feature Transformations**:

| Transformation | Features | Rationale |
|----------------|----------|-----------|
| **Log transform** | Booking Value, Ride Distance | Handle right-skewed distributions |
| **Binning** | VTAT, CTAT, Hour | Create meaningful categories |
| **One-hot encoding** | Vehicle Type, Payment Method | Categorical to numerical |
| **Target encoding** | Pickup/Drop Location | High cardinality categoricals |
| **Cyclical encoding** | Hour, Day of week | Preserve cyclical nature |

**Cyclical Encoding for Time**:
```python
import numpy as np

# Hour encoding (cyclical)
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)

# Day of week encoding (cyclical)
df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
```

**Interaction Features**:
```python
# Vehicle type × Time interactions
df['premium_rush_hour'] = ((df['Vehicle Type'].isin(['Premier Sedan', 'UberXL'])) & 
                           (df['is_peak_hour'])).astype(int)

# VTAT × Vehicle type
df['auto_long_wait'] = ((df['Vehicle Type'] == 'Auto') & 
                        (df['Avg VTAT'] > 10)).astype(int)
```

### 3.9 Identify extra data that would be useful

**Additional Data That Would Improve Model**:

| Data Type | Description | Expected Impact |
|-----------|-------------|-----------------|
| **Weather data** | Temperature, rain, conditions at booking time | High - weather affects cancellations |
| **Traffic data** | Real-time traffic conditions | High - affects VTAT accuracy |
| **Historical customer behavior** | Past cancellation rate, booking frequency | High - repeat behavior patterns |
| **Driver history** | Driver cancellation rate, rating history | High - driver reliability |
| **Surge pricing** | Active surge multiplier | Medium - price sensitivity |
| **App session data** | Time spent booking, changes made | Medium - customer commitment |
| **Event data** | Local events, holidays | Medium - demand patterns |
| **Competitor pricing** | Lyft/Ola prices at booking time | Low - switching behavior |

### 3.10 Document what you have learned

**Key EDA Findings Summary**:

1. **Target Distribution**:
   - 25% overall cancellation rate (37,430 cancelled bookings)
   - Customer cancellations (19.15%) dominate over driver cancellations (7.45%)
   - Moderate class imbalance - manageable with standard techniques

2. **Temporal Patterns** (to verify):
   - Hypothesis: Peak hours may have different cancellation patterns
   - Hypothesis: Weekend vs. weekday differences
   - Late night bookings may have higher cancellation risk

3. **Vehicle Type Insights**:
   - UberXL has highest success rate (92.2%)
   - Premium vehicles may attract more committed customers
   - Budget options (Auto, eBike) may have higher price sensitivity

4. **Wait Time (VTAT) Impact**:
   - Expected strong predictor of cancellation
   - "Driver Not Moving" is a top cancellation reason (22.2%)
   - Long VTAT likely leads to customer frustration and cancellation

5. **Payment Method Patterns**:
   - UPI dominates (40%) - digital-first customer base
   - Cash users (25%) may have different behavior patterns
   - Payment method could indicate customer segment

6. **Cancellation Reasons** (for understanding, not features):
   - Customer: Wrong Address (22.5%), Driver Issues (22.4%), Driver Not Moving (22.2%)
   - Driver: Fairly evenly distributed across reasons (~25% each)
   - Suggests both customer-side and driver-side interventions needed

7. **Data Quality**:
   - Comprehensive coverage with minimal missing values
   - Standardized categories
   - Full year temporal coverage

**Recommendations for Modeling**:
- Focus on VTAT as key predictor
- Create time-based features (hour, day, peak indicators)
- Use vehicle type as important categorical feature
- Handle location features with target encoding (high cardinality)
- Consider separate models for customer vs. driver cancellation

---

## 4. Prepare the Data

> **Notes**:
> - Work on copies of the data (keep the original dataset intact).
> - Write functions for all data transformations you apply, for five reasons:
>   - So you can easily prepare the data the next time you get a fresh dataset
>   - So you can apply these transformations in future projects
>   - To clean and prepare the test set
>   - To clean and prepare new data instances once your solution is live
>   - To make it easy to treat your preparation choices as hyperparameters

### 4.1 Data Cleaning

#### 4.1.1 Handle Missing Values

**Missing Value Strategy**:

| Column | Expected Missing | Strategy |
|--------|------------------|----------|
| Date, Time | None | Error if missing |
| Booking ID, Customer ID | None | Error if missing |
| Booking Status | None | Error if missing (target) |
| Vehicle Type | Rare | Mode imputation |
| Pickup/Drop Location | Rare | "Unknown" category |
| Avg VTAT, Avg CTAT | Some (cancelled rides) | Median imputation or flag |
| Booking Value | Some (cancelled rides) | Median imputation or 0 |
| Ride Distance | Some (cancelled rides) | Median imputation or 0 |
| Cancellation Reasons | Many (non-cancelled) | Expected - not used as features |
| Ratings | Many (cancelled rides) | Not used as features |
| Payment Method | Rare | Mode imputation |

**Implementation**:
```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

def handle_missing_values(df):
    """
    Handle missing values in the dataset.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with missing values handled
    """
    df = df.copy()
    
    # Critical columns - should not have missing values
    critical_cols = ['Date', 'Time', 'Booking ID', 'Booking Status']
    for col in critical_cols:
        if df[col].isna().any():
            raise ValueError(f"Critical column {col} has missing values")
    
    # Numerical columns - median imputation
    numerical_cols = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance']
    for col in numerical_cols:
        if col in df.columns:
            median_val = df[col].median()
            df[col] = df[col].fillna(median_val)
            # Create missing indicator flag
            df[f'{col}_was_missing'] = df[col].isna().astype(int)
    
    # Categorical columns - mode imputation or 'Unknown'
    categorical_cols = ['Vehicle Type', 'Payment Method']
    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].fillna(df[col].mode()[0])
    
    # Location columns - 'Unknown' category
    location_cols = ['Pickup Location', 'Drop Location']
    for col in location_cols:
        if col in df.columns:
            df[col] = df[col].fillna('Unknown')
    
    return df
```

#### 4.1.2 Handle Outliers

**Outlier Detection and Treatment**:

| Column | Outlier Definition | Treatment |
|--------|-------------------|-----------|
| Avg VTAT | > 60 minutes | Cap at 99th percentile |
| Avg CTAT | > 180 minutes | Cap at 99th percentile |
| Booking Value | > 99th percentile | Cap at 99th percentile |
| Ride Distance | > 100 km | Cap at 99th percentile |

**Implementation**:
```python
def handle_outliers(df, columns, method='cap', threshold=0.99):
    """
    Handle outliers using capping or removal.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
    columns : list
        Columns to check for outliers
    method : str
        'cap' to cap at percentile, 'remove' to drop rows
    threshold : float
        Percentile threshold for outlier detection
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with outliers handled
    """
    df = df.copy()
    
    for col in columns:
        if col not in df.columns:
            continue
            
        upper_limit = df[col].quantile(threshold)
        lower_limit = df[col].quantile(1 - threshold)
        
        if method == 'cap':
            df[col] = df[col].clip(lower=lower_limit, upper=upper_limit)
        elif method == 'remove':
            df = df[(df[col] >= lower_limit) & (df[col] <= upper_limit)]
    
    return df

# Apply to numerical columns
outlier_columns = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance']
df_clean = handle_outliers(df, outlier_columns, method='cap')
```

#### 4.1.3 Handle Duplicates

```python
def remove_duplicates(df, subset=None):
    """
    Remove duplicate rows.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
    subset : list, optional
        Columns to consider for duplicate detection
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with duplicates removed
    """
    initial_count = len(df)
    
    if subset:
        df = df.drop_duplicates(subset=subset, keep='first')
    else:
        df = df.drop_duplicates(keep='first')
    
    removed_count = initial_count - len(df)
    print(f"Removed {removed_count} duplicate rows ({removed_count/initial_count*100:.2f}%)")
    
    return df

# Remove duplicates based on Booking ID
df_clean = remove_duplicates(df, subset=['Booking ID'])
```

### 4.2 Feature Selection

#### 4.2.1 Drop Irrelevant Attributes

**Columns to Drop**:

| Column | Reason |
|--------|--------|
| Booking ID | Identifier, no predictive value |
| Reason for cancelling by Customer | Post-event data (data leakage) |
| Driver Cancellation Reason | Post-event data (data leakage) |
| Incomplete Rides Reason | Post-event data (data leakage) |
| Driver Ratings | Post-ride data (not available at prediction time) |
| Customer Rating | Post-ride data (not available at prediction time) |
| Cancelled Rides by Customer | Target component (leakage) |
| Cancelled Rides by Driver | Target component (leakage) |
| Incomplete Rides | Target component (leakage) |

**Implementation**:
```python
def select_features(df):
    """
    Select features available at prediction time.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with only usable features
    """
    # Columns to drop (not available at prediction time or identifiers)
    drop_columns = [
        'Booking ID',
        'Reason for cancelling by Customer',
        'Driver Cancellation Reason',
        'Incomplete Rides Reason',
        'Driver Ratings',
        'Customer Rating',
        'Cancelled Rides by Customer',
        'Cancelled Rides by Driver',
        'Incomplete Rides'
    ]
    
    # Keep only columns that exist in dataframe
    drop_columns = [col for col in drop_columns if col in df.columns]
    
    return df.drop(columns=drop_columns)
```

**Features to Keep**:
- Date, Time → For temporal feature engineering
- Customer ID → For aggregated customer features
- Vehicle Type → Categorical predictor
- Pickup Location, Drop Location → Location features
- Avg VTAT → Key predictor (wait time)
- Avg CTAT → Trip duration estimate
- Booking Value → Price information
- Ride Distance → Trip length
- Payment Method → Customer segment indicator
- Booking Status → Target variable source

### 4.3 Feature Engineering

#### 4.3.1 Temporal Features

```python
def create_temporal_features(df):
    """
    Create time-based features from Date and Time columns.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe with Date and Time columns
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with temporal features added
    """
    df = df.copy()
    
    # Ensure datetime types
    df['Date'] = pd.to_datetime(df['Date'])
    
    # Extract date components
    df['year'] = df['Date'].dt.year
    df['month'] = df['Date'].dt.month
    df['day'] = df['Date'].dt.day
    df['dayofweek'] = df['Date'].dt.dayofweek  # 0=Monday, 6=Sunday
    df['week_of_year'] = df['Date'].dt.isocalendar().week
    
    # Extract time components
    if df['Time'].dtype == 'object':
        df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.time
    
    df['hour'] = df['Time'].apply(lambda x: x.hour if pd.notna(x) else 0)
    df['minute'] = df['Time'].apply(lambda x: x.minute if pd.notna(x) else 0)
    
    # Binary features
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
    df['is_morning_rush'] = ((df['hour'] >= 7) & (df['hour'] <= 10)).astype(int)
    df['is_evening_rush'] = ((df['hour'] >= 17) & (df['hour'] <= 21)).astype(int)
    df['is_peak_hour'] = (df['is_morning_rush'] | df['is_evening_rush']).astype(int)
    df['is_late_night'] = ((df['hour'] >= 23) | (df['hour'] <= 5)).astype(int)
    
    # Cyclical encoding for hour and day (preserves circular nature)
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    
    return df
```

#### 4.3.2 Wait Time Features

```python
def create_vtat_features(df):
    """
    Create features based on Vehicle Time to Arrival (VTAT).
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe with Avg VTAT column
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with VTAT-based features
    """
    df = df.copy()
    
    # VTAT buckets
    df['vtat_bucket'] = pd.cut(
        df['Avg VTAT'],
        bins=[0, 5, 10, 15, 20, float('inf')],
        labels=['0-5min', '5-10min', '10-15min', '15-20min', '20+min']
    )
    
    # Binary flags
    df['is_long_wait'] = (df['Avg VTAT'] > 10).astype(int)
    df['is_very_long_wait'] = (df['Avg VTAT'] > 15).astype(int)
    
    # Relative VTAT (compared to average)
    mean_vtat = df['Avg VTAT'].mean()
    df['vtat_relative'] = df['Avg VTAT'] / mean_vtat
    
    return df
```

#### 4.3.3 Distance and Value Features

```python
def create_trip_features(df):
    """
    Create features based on ride distance and booking value.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with trip-based features
    """
    df = df.copy()
    
    # Distance buckets
    df['distance_bucket'] = pd.cut(
        df['Ride Distance'],
        bins=[0, 5, 10, 20, 30, float('inf')],
        labels=['very_short', 'short', 'medium', 'long', 'very_long']
    )
    
    # Value per km (price density)
    df['value_per_km'] = df['Booking Value'] / (df['Ride Distance'] + 0.1)  # Avoid division by zero
    
    # Log transformations (handle skewness)
    df['log_distance'] = np.log1p(df['Ride Distance'])
    df['log_value'] = np.log1p(df['Booking Value'])
    
    # Trip efficiency (time vs distance)
    df['trip_efficiency'] = df['Ride Distance'] / (df['Avg CTAT'] + 0.1)
    
    return df
```

#### 4.3.4 Location Features

```python
def create_location_features(df, min_count=100):
    """
    Create location-based features.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
    min_count : int
        Minimum bookings for a location to be kept as-is
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with location features
    """
    df = df.copy()
    
    # Same pickup and drop location
    df['is_same_area'] = (df['Pickup Location'] == df['Drop Location']).astype(int)
    
    # Create route feature
    df['route'] = df['Pickup Location'] + ' -> ' + df['Drop Location']
    
    # Location frequency (popularity)
    pickup_counts = df['Pickup Location'].value_counts()
    drop_counts = df['Drop Location'].value_counts()
    
    df['pickup_popularity'] = df['Pickup Location'].map(pickup_counts)
    df['drop_popularity'] = df['Drop Location'].map(drop_counts)
    
    # Group rare locations
    rare_pickups = pickup_counts[pickup_counts < min_count].index
    rare_drops = drop_counts[drop_counts < min_count].index
    
    df['Pickup Location Grouped'] = df['Pickup Location'].apply(
        lambda x: 'Other' if x in rare_pickups else x
    )
    df['Drop Location Grouped'] = df['Drop Location'].apply(
        lambda x: 'Other' if x in rare_drops else x
    )
    
    return df
```

#### 4.3.5 Interaction Features

```python
def create_interaction_features(df):
    """
    Create interaction features between existing features.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with interaction features
    """
    df = df.copy()
    
    # Vehicle type × Time interactions
    df['premium_vehicle'] = df['Vehicle Type'].isin(['Premier Sedan', 'UberXL']).astype(int)
    df['budget_vehicle'] = df['Vehicle Type'].isin(['Auto', 'eBike/Bike']).astype(int)
    
    df['premium_rush_hour'] = (df['premium_vehicle'] & df['is_peak_hour']).astype(int)
    df['budget_long_wait'] = (df['budget_vehicle'] & df['is_long_wait']).astype(int)
    
    # Payment × Vehicle interactions
    df['cash_budget'] = ((df['Payment Method'] == 'Cash') & df['budget_vehicle']).astype(int)
    
    # Time × Wait interactions
    df['late_night_long_wait'] = (df['is_late_night'] & df['is_long_wait']).astype(int)
    df['rush_hour_long_wait'] = (df['is_peak_hour'] & df['is_long_wait']).astype(int)
    
    # Distance × Value interactions
    df['long_trip_high_value'] = (
        (df['distance_bucket'].isin(['long', 'very_long'])) & 
        (df['Booking Value'] > df['Booking Value'].median())
    ).astype(int)
    
    return df
```

#### 4.3.6 Target Variable Creation

```python
def create_target_variable(df):
    """
    Create the target variable for classification.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe with Booking Status column
        
    Returns:
    --------
    pd.DataFrame
        Dataframe with target variable
    """
    df = df.copy()
    
    # Binary target: 1 = Cancelled, 0 = Completed
    cancelled_statuses = ['Cancelled by Customer', 'Cancelled by Driver']
    df['is_cancelled'] = df['Booking Status'].isin(cancelled_statuses).astype(int)
    
    # Multi-class target (optional)
    status_mapping = {
        'Completed': 0,
        'Cancelled by Customer': 1,
        'Cancelled by Driver': 2
    }
    df['cancellation_type'] = df['Booking Status'].map(status_mapping)
    
    # Filter out incomplete rides if needed
    # df = df[df['Booking Status'] != 'Incomplete']
    
    return df
```

### 4.4 Feature Scaling

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

def scale_numerical_features(df, columns, method='standard'):
    """
    Scale numerical features.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
    columns : list
        Numerical columns to scale
    method : str
        'standard' (z-score), 'minmax' (0-1), or 'robust' (IQR-based)
        
    Returns:
    --------
    pd.DataFrame, scaler
        Scaled dataframe and fitted scaler
    """
    df = df.copy()
    
    if method == 'standard':
        scaler = StandardScaler()
    elif method == 'minmax':
        scaler = MinMaxScaler()
    elif method == 'robust':
        scaler = RobustScaler()
    else:
        raise ValueError(f"Unknown scaling method: {method}")
    
    # Fit and transform
    df[columns] = scaler.fit_transform(df[columns])
    
    return df, scaler

# Columns to scale
scale_columns = [
    'Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance',
    'value_per_km', 'trip_efficiency', 'vtat_relative'
]
```

### 4.5 Categorical Encoding

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from category_encoders import TargetEncoder

def encode_categorical_features(df, target_col='is_cancelled'):
    """
    Encode categorical features for modeling.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
    target_col : str
        Target column name for target encoding
        
    Returns:
    --------
    pd.DataFrame, dict
        Encoded dataframe and encoders dictionary
    """
    df = df.copy()
    encoders = {}
    
    # One-hot encoding for low cardinality categoricals
    onehot_cols = ['Vehicle Type', 'Payment Method', 'vtat_bucket', 'distance_bucket']
    for col in onehot_cols:
        if col in df.columns:
            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
            df = pd.concat([df, dummies], axis=1)
            df = df.drop(columns=[col])
    
    # Target encoding for high cardinality categoricals (locations)
    target_encode_cols = ['Pickup Location Grouped', 'Drop Location Grouped', 'route']
    for col in target_encode_cols:
        if col in df.columns:
            encoder = TargetEncoder(cols=[col])
            df[f'{col}_encoded'] = encoder.fit_transform(df[col], df[target_col])
            encoders[col] = encoder
            df = df.drop(columns=[col])
    
    return df, encoders
```

### 4.6 Complete Preprocessing Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

def create_preprocessing_pipeline():
    """
    Create a complete preprocessing pipeline.
    
    Returns:
    --------
    sklearn.pipeline.Pipeline
        Complete preprocessing pipeline
    """
    # Define column groups
    numerical_features = [
        'Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance',
        'hour', 'dayofweek', 'month'
    ]
    
    categorical_features = [
        'Vehicle Type', 'Payment Method'
    ]
    
    # Numerical transformer
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Categorical transformer
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])
    
    # Combine transformers
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'  # Drop columns not specified
    )
    
    return preprocessor

# Usage
preprocessor = create_preprocessing_pipeline()
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)
```

### 4.7 Master Preprocessing Function

```python
def preprocess_data(df, is_training=True, encoders=None, scaler=None):
    """
    Master function to preprocess the data.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Raw input dataframe
    is_training : bool
        Whether this is training data (fit transformers) or inference data (transform only)
    encoders : dict, optional
        Pre-fitted encoders (for inference)
    scaler : object, optional
        Pre-fitted scaler (for inference)
        
    Returns:
    --------
    pd.DataFrame, dict, object
        Processed dataframe, encoders, and scaler
    """
    # Step 1: Handle missing values
    df = handle_missing_values(df)
    
    # Step 2: Handle outliers
    df = handle_outliers(df, ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance'])
    
    # Step 3: Remove duplicates (training only)
    if is_training:
        df = remove_duplicates(df, subset=['Booking ID'])
    
    # Step 4: Create target variable
    df = create_target_variable(df)
    
    # Step 5: Feature selection
    df = select_features(df)
    
    # Step 6: Feature engineering
    df = create_temporal_features(df)
    df = create_vtat_features(df)
    df = create_trip_features(df)
    df = create_location_features(df)
    df = create_interaction_features(df)
    
    # Step 7: Encode categoricals
    if is_training:
        df, encoders = encode_categorical_features(df)
    else:
        # Use pre-fitted encoders for inference
        df = apply_encoders(df, encoders)
    
    # Step 8: Scale numerical features
    scale_cols = ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance',
                  'value_per_km', 'trip_efficiency', 'vtat_relative']
    
    if is_training:
        df, scaler = scale_numerical_features(df, scale_cols)
    else:
        df[scale_cols] = scaler.transform(df[scale_cols])
    
    # Step 9: Drop original columns no longer needed
    drop_cols = ['Date', 'Time', 'Customer ID', 'Booking Status', 
                 'Pickup Location', 'Drop Location']
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')
    
    return df, encoders, scaler
```

---

## 5. Short-List Promising Models

> **Notes**:
> - If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time.
> - Try to automate these steps as much as possible.

### 5.1 Train Quick and Dirty Models from Different Categories

**Model Selection Rationale**:

| Model | Category | Why Include | Strengths | Weaknesses |
|-------|----------|-------------|-----------|------------|
| **Logistic Regression** | Linear | Baseline, interpretable | Fast, interpretable coefficients | Assumes linearity |
| **Random Forest** | Ensemble (Bagging) | Robust, handles imbalance | No scaling needed, feature importance | Can overfit, slow inference |
| **XGBoost** | Ensemble (Boosting) | State-of-the-art performance | Handles imbalance, regularization | Requires tuning |
| **LightGBM** | Ensemble (Boosting) | Fast training, large datasets | Memory efficient, categorical support | Sensitive to overfitting |
| **SVM** | Kernel | Non-linear boundaries | Effective in high dimensions | Slow on large datasets |
| **Neural Network** | Deep Learning | Complex patterns | Flexible architecture | Requires more data, tuning |

**Implementation**:
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier

# Define models with default parameters
models = {
    'Logistic Regression': LogisticRegression(
        random_state=42, max_iter=1000, class_weight='balanced'
    ),
    'Random Forest': RandomForestClassifier(
        n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1
    ),
    'XGBoost': XGBClassifier(
        n_estimators=100, random_state=42, scale_pos_weight=3,  # ~ratio of neg/pos
        use_label_encoder=False, eval_metric='logloss'
    ),
    'LightGBM': LGBMClassifier(
        n_estimators=100, random_state=42, class_weight='balanced', verbose=-1
    ),
    'SVM': SVC(
        random_state=42, class_weight='balanced', probability=True
    ),
    'MLP': MLPClassifier(
        hidden_layer_sizes=(100, 50), random_state=42, max_iter=500
    )
}
```

### 5.2 Measure and Compare Performance

**Cross-Validation Strategy**:
```python
from sklearn.model_selection import StratifiedKFold, cross_validate

def evaluate_models(models, X, y, cv=5):
    """
    Evaluate multiple models using cross-validation.
    """
    results = {}
    cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)
    
    scoring = {
        'accuracy': 'accuracy',
        'precision': 'precision',
        'recall': 'recall',
        'f1': 'f1',
        'roc_auc': 'roc_auc'
    }
    
    for name, model in models.items():
        print(f"Evaluating {name}...")
        cv_results = cross_validate(
            model, X, y, cv=cv_strategy, scoring=scoring,
            return_train_score=True, n_jobs=-1
        )
        
        results[name] = {
            'accuracy': (cv_results['test_accuracy'].mean(), cv_results['test_accuracy'].std()),
            'precision': (cv_results['test_precision'].mean(), cv_results['test_precision'].std()),
            'recall': (cv_results['test_recall'].mean(), cv_results['test_recall'].std()),
            'f1': (cv_results['test_f1'].mean(), cv_results['test_f1'].std()),
            'roc_auc': (cv_results['test_roc_auc'].mean(), cv_results['test_roc_auc'].std()),
        }
    
    return results

# Expected results format
# | Model | Accuracy | Precision | Recall | F1 | AUC-ROC |
```

**Handling Class Imbalance**:
```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

# Option 1: SMOTE oversampling
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Option 2: Class weights (built into models)
# Already included in model definitions above

# Option 3: Combined sampling
sampling_pipeline = ImbPipeline([
    ('over', SMOTE(sampling_strategy=0.5)),
    ('under', RandomUnderSampler(sampling_strategy=0.8))
])
```

### 5.3 Analyze Most Significant Variables

```python
def get_feature_importance(model, feature_names, model_type='tree'):
    """
    Extract feature importance from trained model.
    """
    if model_type == 'tree':
        importance = model.feature_importances_
    elif model_type == 'linear':
        importance = np.abs(model.coef_[0])
    
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    return importance_df

# For XGBoost/LightGBM - SHAP values
import shap

def explain_model(model, X_sample):
    """
    Generate SHAP explanations for model predictions.
    """
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_sample)
    
    # Summary plot
    shap.summary_plot(shap_values, X_sample)
    
    return shap_values
```

**Expected Key Predictors** (hypotheses to verify):
1. `Avg VTAT` - Wait time (strongest expected predictor)
2. `is_peak_hour` - Rush hour indicator
3. `Vehicle Type` - Budget vs. premium
4. `is_late_night` - Late night bookings
5. `Booking Value` - Price sensitivity

### 5.4 Analyze Types of Errors

```python
from sklearn.metrics import confusion_matrix, classification_report

def analyze_errors(model, X_test, y_test, feature_names):
    """
    Analyze prediction errors to understand model weaknesses.
    """
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(f"TN: {cm[0,0]}, FP: {cm[0,1]}")
    print(f"FN: {cm[1,0]}, TP: {cm[1,1]}")
    
    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Completed', 'Cancelled']))
    
    # Analyze false negatives (missed cancellations)
    fn_mask = (y_test == 1) & (y_pred == 0)
    fn_samples = X_test[fn_mask]
    print(f"\nFalse Negatives: {fn_mask.sum()} ({fn_mask.mean()*100:.1f}%)")
    
    # Analyze false positives (false alarms)
    fp_mask = (y_test == 0) & (y_pred == 1)
    fp_samples = X_test[fp_mask]
    print(f"False Positives: {fp_mask.sum()} ({fp_mask.mean()*100:.1f}%)")
    
    return {'fn_samples': fn_samples, 'fp_samples': fp_samples}
```

### 5.5 Quick Round of Feature Selection and Engineering

```python
from sklearn.feature_selection import SelectFromModel, RFE

def select_features_from_model(model, X, y, threshold='median'):
    """
    Select features based on model importance.
    """
    selector = SelectFromModel(model, threshold=threshold)
    selector.fit(X, y)
    
    selected_features = X.columns[selector.get_support()].tolist()
    print(f"Selected {len(selected_features)} features out of {X.shape[1]}")
    
    return selected_features, selector

# Recursive Feature Elimination
def rfe_selection(model, X, y, n_features=20):
    """
    Select top N features using RFE.
    """
    rfe = RFE(model, n_features_to_select=n_features)
    rfe.fit(X, y)
    
    selected_features = X.columns[rfe.support_].tolist()
    return selected_features, rfe
```

### 5.6 Short-List Top Models

**Model Selection Criteria**:

| Criterion | Weight | Description |
|-----------|--------|-------------|
| Recall | 30% | Ability to catch cancellations |
| F1-Score | 25% | Balance of precision and recall |
| AUC-ROC | 20% | Overall discrimination ability |
| Training Time | 10% | Practical for retraining |
| Interpretability | 15% | Ability to explain predictions |

**Expected Short-List**:
1. **XGBoost** - Best overall performance
2. **LightGBM** - Fast training, competitive performance
3. **Random Forest** - Robust, good interpretability

---

## 6. Fine-Tune the System

> **Notes**:
> - Use as much data as possible for this step.
> - Automate what you can.
> - Treat data transformation choices as hyperparameters.

### 6.1 Hyperparameter Tuning

**XGBoost Hyperparameter Space**:
```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

xgb_param_space = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'min_child_weight': randint(1, 10),
    'gamma': uniform(0, 0.5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1),
    'scale_pos_weight': uniform(2, 4)  # For imbalanced data
}

# Randomized search
xgb_search = RandomizedSearchCV(
    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    param_distributions=xgb_param_space,
    n_iter=50,
    cv=StratifiedKFold(n_splits=5),
    scoring='f1',
    random_state=42,
    n_jobs=-1,
    verbose=1
)

xgb_search.fit(X_train, y_train)
print(f"Best F1 Score: {xgb_search.best_score_:.4f}")
print(f"Best Parameters: {xgb_search.best_params_}")
```

**LightGBM Hyperparameter Space**:
```python
lgbm_param_space = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 15),
    'learning_rate': uniform(0.01, 0.3),
    'num_leaves': randint(20, 100),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'min_child_samples': randint(10, 100),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}
```

**Random Forest Hyperparameter Space**:
```python
rf_param_space = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(5, 30),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False]
}
```

### 6.2 Threshold Optimization

```python
from sklearn.metrics import precision_recall_curve

def optimize_threshold(model, X_val, y_val, target_recall=0.7):
    """
    Find optimal probability threshold for desired recall.
    """
    y_prob = model.predict_proba(X_val)[:, 1]
    
    precisions, recalls, thresholds = precision_recall_curve(y_val, y_prob)
    
    # Find threshold that achieves target recall
    idx = np.argmin(np.abs(recalls - target_recall))
    optimal_threshold = thresholds[idx] if idx < len(thresholds) else 0.5
    
    print(f"Optimal threshold for {target_recall:.0%} recall: {optimal_threshold:.3f}")
    print(f"Precision at this threshold: {precisions[idx]:.3f}")
    
    return optimal_threshold

# Apply custom threshold
def predict_with_threshold(model, X, threshold=0.5):
    """
    Make predictions using custom probability threshold.
    """
    y_prob = model.predict_proba(X)[:, 1]
    return (y_prob >= threshold).astype(int)
```

### 6.3 Ensemble Methods

```python
from sklearn.ensemble import VotingClassifier, StackingClassifier

# Voting ensemble
voting_clf = VotingClassifier(
    estimators=[
        ('xgb', xgb_search.best_estimator_),
        ('lgbm', lgbm_search.best_estimator_),
        ('rf', rf_search.best_estimator_)
    ],
    voting='soft',  # Use probabilities
    weights=[0.4, 0.35, 0.25]  # Based on individual performance
)

# Stacking ensemble
stacking_clf = StackingClassifier(
    estimators=[
        ('xgb', xgb_search.best_estimator_),
        ('lgbm', lgbm_search.best_estimator_),
        ('rf', rf_search.best_estimator_)
    ],
    final_estimator=LogisticRegression(random_state=42),
    cv=5
)
```

### 6.4 Final Model Evaluation on Test Set

```python
def final_evaluation(model, X_test, y_test, threshold=0.5):
    """
    Comprehensive evaluation on held-out test set.
    """
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, 
        f1_score, roc_auc_score, average_precision_score,
        confusion_matrix, classification_report
    )
    
    # Predictions
    y_pred = predict_with_threshold(model, X_test, threshold)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    # Metrics
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'AUC-ROC': roc_auc_score(y_test, y_prob),
        'Average Precision': average_precision_score(y_test, y_prob)
    }
    
    print("=" * 50)
    print("FINAL TEST SET EVALUATION")
    print("=" * 50)
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")
    
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Completed', 'Cancelled']))
    
    return metrics

# Run final evaluation (ONLY ONCE!)
final_metrics = final_evaluation(best_model, X_test, y_test, optimal_threshold)
```

**Expected Final Performance** (targets):

| Metric | Minimum | Target | Stretch |
|--------|---------|--------|---------|
| Recall | 60% | 70% | 80% |
| Precision | 50% | 60% | 70% |
| F1-Score | 0.55 | 0.65 | 0.75 |
| AUC-ROC | 0.70 | 0.75 | 0.85 |

---

## 7. Present Your Solution

### 7.1 Document What You Have Done

**Project Documentation Structure**:

```
uber-analysis/
├── README.md                    # Project overview and quick start
├── PROJECT_WALKTHROUGH.md       # This document - methodology
├── DATASET_INFO.md              # Data dictionary and statistics
├── notebooks/
│   ├── 01_ingest_data.ipynb     # Data acquisition
│   ├── 02_eda.ipynb             # Exploratory analysis with findings
│   ├── 03_preprocessing.ipynb   # Data preparation pipeline
│   └── 04_modeling.ipynb        # Model training and evaluation
├── src/
│   ├── data/
│   │   └── preprocessing.py     # Reusable preprocessing functions
│   ├── features/
│   │   └── engineering.py       # Feature engineering functions
│   └── models/
│       ├── train.py             # Training script
│       └── predict.py           # Inference script
└── reports/
    ├── final_report.pdf         # Executive summary
    └── figures/                  # Key visualizations
```

**Documentation Checklist**:
- [ ] Data source and acquisition process
- [ ] EDA findings and insights
- [ ] Feature engineering rationale
- [ ] Model selection process
- [ ] Hyperparameter tuning results
- [ ] Final model performance
- [ ] Business recommendations
- [ ] Limitations and assumptions

### 7.2 Create a Nice Presentation

**Presentation Outline** (10-15 slides):

| Slide | Content | Key Points |
|-------|---------|------------|
| 1 | Title | Uber Ride Cancellation Prediction |
| 2 | Business Problem | 25% cancellation rate, revenue impact |
| 3 | Solution Overview | ML-based early warning system |
| 4 | Data Overview | 148K bookings, 20 features, 2024 data |
| 5 | Key EDA Insights | Top cancellation drivers |
| 6 | Feature Engineering | Time, location, wait time features |
| 7 | Model Comparison | Performance across models |
| 8 | Final Model | XGBoost/ensemble with metrics |
| 9 | Feature Importance | SHAP analysis, key predictors |
| 10 | Business Impact | Projected savings, intervention ROI |
| 11 | Recommendations | Actionable next steps |
| 12 | Limitations | Data constraints, assumptions |
| 13 | Next Steps | Deployment plan, monitoring |

### 7.3 Explain Why Your Solution Achieves the Business Objective

**Business Value Proposition**:

1. **Revenue Recovery**:
   - Current cancellations: 37,430 bookings/year
   - Target reduction: 10% (3,743 saved bookings)
   - Average booking value: ~$20
   - **Potential annual savings: ~$75,000**

2. **Customer Experience Improvement**:
   - Proactive communication reduces frustration
   - Faster driver assignment for high-risk bookings
   - Improved app reliability perception

3. **Operational Efficiency**:
   - Reduced wasted driver time
   - Better resource allocation
   - Data-driven decision making

**ROI Calculation**:
```
Investment:
- Model development: ~40 hours @ $100/hr = $4,000
- Infrastructure: ~$500/month = $6,000/year
- Maintenance: ~10 hrs/month @ $100/hr = $12,000/year
Total Annual Cost: ~$22,000

Returns:
- Saved bookings revenue: $75,000
- Customer retention value: $25,000 (estimated)
- Operational savings: $10,000
Total Annual Benefit: ~$110,000

ROI = (110,000 - 22,000) / 22,000 = 400%
```

### 7.4 Present Interesting Points Noticed Along the Way

**Key Discoveries**:

1. **Wait Time is Critical**:
   - VTAT (Vehicle Time to Arrival) is the strongest predictor
   - Bookings with VTAT > 15 minutes have 2x cancellation rate
   - "Driver Not Moving" accounts for 22.2% of customer cancellations

2. **Vehicle Type Matters**:
   - UberXL has highest completion rate (92.2%)
   - Budget options (Auto, eBike) have higher cancellation sensitivity
   - Premium customers show more commitment

3. **Temporal Patterns**:
   - Late night bookings (11 PM - 5 AM) show elevated risk
   - Rush hour has different patterns than off-peak
   - Weekend vs. weekday differences exist

4. **Payment Method Insights**:
   - UPI dominates (40%) indicating digital-first customers
   - Cash users may have different cancellation patterns
   - Payment method correlates with customer segment

5. **Cancellation Symmetry**:
   - Customer cancellations (19%) > Driver cancellations (7%)
   - Different intervention strategies needed for each
   - Driver-side issues often capacity or vehicle related

**What Worked**:
- SMOTE for handling class imbalance
- Cyclical encoding for time features
- Target encoding for high-cardinality locations
- XGBoost with tuned hyperparameters

**What Didn't Work**:
- Simple rule-based approach (insufficient accuracy)
- Using post-ride ratings as features (data leakage)
- Ignoring class imbalance (biased toward majority class)

### 7.5 List Assumptions and System Limitations

**Assumptions**:

| Assumption | Risk Level | Mitigation |
|------------|------------|------------|
| 2024 patterns persist | Medium | Regular retraining |
| Intervention can prevent cancellations | Medium | A/B testing |
| Features available at booking time | Low | Data pipeline validation |
| Cost structure remains stable | Low | Periodic review |

**Limitations**:

1. **Data Limitations**:
   - Single year of data (no multi-year trends)
   - NCR region only (may not generalize to other cities)
   - No weather or traffic data
   - No customer history features

2. **Model Limitations**:
   - Cannot predict completely random cancellations
   - May not capture sudden behavioral shifts
   - Requires regular retraining
   - Black-box nature of ensemble models

3. **Operational Limitations**:
   - Intervention capacity constraints
   - Customer fatigue from too many messages
   - Driver assignment flexibility limits

### 7.6 Key Findings Through Beautiful Visualizations

**Recommended Visualizations**:

1. **Cancellation Rate Dashboard**:
   - Overall rate gauge chart
   - Trend over time (line chart)
   - By vehicle type (bar chart)

2. **Feature Importance Plot**:
   - SHAP summary plot
   - Top 10 features bar chart

3. **Model Performance**:
   - ROC curve comparison
   - Precision-Recall curve
   - Confusion matrix heatmap

4. **Business Impact**:
   - Revenue impact waterfall chart
   - Intervention success funnel

**Key Statements**:
> "Wait time (VTAT) is the #1 predictor of cancellation - every additional minute of wait increases cancellation probability by X%"

> "UberXL bookings are 15% less likely to be cancelled than Auto bookings"

> "Our model can identify 70% of cancellations at booking time with 60% precision"

---

## 8. Launch!

### 8.1 Get Your Solution Ready for Production

**Production Checklist**:

| Component | Status | Description |
|-----------|--------|-------------|
| Model serialization | Pending | Save model with joblib/pickle |
| Preprocessing pipeline | Pending | Sklearn pipeline serialization |
| API endpoint | Pending | FastAPI/Flask REST API |
| Input validation | Pending | Pydantic schemas |
| Error handling | Pending | Graceful degradation |
| Logging | Pending | Structured logging |
| Unit tests | Pending | pytest coverage |
| Integration tests | Pending | End-to-end testing |
| Documentation | Pending | API docs, runbooks |

**Model Serialization**:
```python
import joblib
from datetime import datetime

def save_model(model, preprocessor, metadata):
    """
    Save model and preprocessing pipeline for production.
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Save model
    joblib.dump(model, f'models/cancellation_model_{timestamp}.joblib')
    
    # Save preprocessor
    joblib.dump(preprocessor, f'models/preprocessor_{timestamp}.joblib')
    
    # Save metadata
    metadata['timestamp'] = timestamp
    metadata['model_version'] = f'v1.0_{timestamp}'
    joblib.dump(metadata, f'models/metadata_{timestamp}.joblib')
    
    return metadata['model_version']
```

**API Endpoint Example**:
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib

app = FastAPI(title="Uber Cancellation Predictor")

class BookingRequest(BaseModel):
    vehicle_type: str
    pickup_location: str
    drop_location: str
    booking_time: str
    avg_vtat: float
    booking_value: float
    payment_method: str
    ride_distance: float

class PredictionResponse(BaseModel):
    cancellation_probability: float
    risk_level: str  # low, medium, high
    recommended_action: str

@app.post("/predict", response_model=PredictionResponse)
def predict_cancellation(booking: BookingRequest):
    """
    Predict cancellation probability for a booking.
    """
    # Load model (in production, load once at startup)
    model = joblib.load('models/cancellation_model.joblib')
    preprocessor = joblib.load('models/preprocessor.joblib')
    
    # Preprocess input
    features = preprocessor.transform(booking.dict())
    
    # Predict
    probability = model.predict_proba(features)[0, 1]
    
    # Determine risk level and action
    if probability >= 0.7:
        risk_level = "high"
        action = "Assign experienced driver, send confirmation"
    elif probability >= 0.4:
        risk_level = "medium"
        action = "Monitor booking, prepare backup driver"
    else:
        risk_level = "low"
        action = "Standard processing"
    
    return PredictionResponse(
        cancellation_probability=probability,
        risk_level=risk_level,
        recommended_action=action
    )
```

### 8.2 Write Monitoring Code

**Performance Monitoring**:
```python
import logging
from datetime import datetime, timedelta
import pandas as pd

logger = logging.getLogger(__name__)

class ModelMonitor:
    """
    Monitor model performance in production.
    """
    
    def __init__(self, alert_threshold=0.1):
        self.alert_threshold = alert_threshold
        self.baseline_metrics = None
        self.predictions_log = []
    
    def log_prediction(self, features, prediction, actual=None):
        """
        Log each prediction for monitoring.
        """
        self.predictions_log.append({
            'timestamp': datetime.now(),
            'features': features,
            'prediction': prediction,
            'actual': actual
        })
    
    def calculate_metrics(self, window_days=7):
        """
        Calculate recent model performance.
        """
        cutoff = datetime.now() - timedelta(days=window_days)
        recent = [p for p in self.predictions_log 
                  if p['timestamp'] > cutoff and p['actual'] is not None]
        
        if not recent:
            return None
        
        y_true = [p['actual'] for p in recent]
        y_pred = [p['prediction'] for p in recent]
        
        from sklearn.metrics import accuracy_score, precision_score, recall_score
        
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred),
            'recall': recall_score(y_true, y_pred),
            'sample_size': len(recent)
        }
    
    def check_drift(self):
        """
        Check for model performance drift.
        """
        current_metrics = self.calculate_metrics()
        
        if current_metrics is None or self.baseline_metrics is None:
            return False
        
        for metric in ['accuracy', 'precision', 'recall']:
            drift = abs(current_metrics[metric] - self.baseline_metrics[metric])
            if drift > self.alert_threshold:
                logger.warning(f"Model drift detected: {metric} changed by {drift:.2%}")
                return True
        
        return False
```

**Input Quality Monitoring**:
```python
def monitor_input_quality(df, expected_schema):
    """
    Monitor quality of incoming data.
    """
    issues = []
    
    # Check for missing columns
    missing_cols = set(expected_schema.keys()) - set(df.columns)
    if missing_cols:
        issues.append(f"Missing columns: {missing_cols}")
    
    # Check for unexpected nulls
    for col, dtype in expected_schema.items():
        if col in df.columns:
            null_rate = df[col].isna().mean()
            if null_rate > 0.1:  # More than 10% nulls
                issues.append(f"High null rate in {col}: {null_rate:.1%}")
    
    # Check for out-of-range values
    if 'Avg VTAT' in df.columns:
        if (df['Avg VTAT'] < 0).any() or (df['Avg VTAT'] > 120).any():
            issues.append("VTAT values out of expected range")
    
    return issues
```

**Alerting**:
```python
def send_alert(message, severity='warning'):
    """
    Send alert to monitoring system.
    """
    # Integration with alerting system (Slack, PagerDuty, email)
    alert = {
        'timestamp': datetime.now().isoformat(),
        'service': 'cancellation-predictor',
        'severity': severity,
        'message': message
    }
    
    logger.warning(f"ALERT [{severity}]: {message}")
    # In production: send to alerting service
    return alert
```

### 8.3 Retrain Models on Fresh Data

**Retraining Schedule**:

| Trigger | Frequency | Description |
|---------|-----------|-------------|
| Scheduled | Monthly | Regular retraining with latest data |
| Performance drift | As needed | When metrics drop below threshold |
| Data drift | As needed | When input distributions change |
| Business change | As needed | New vehicle types, regions, etc. |

**Automated Retraining Pipeline**:
```python
from datetime import datetime
import mlflow

def retrain_model(new_data_path, model_registry='models/'):
    """
    Retrain model with fresh data.
    """
    # Start MLflow run
    with mlflow.start_run(run_name=f"retrain_{datetime.now().strftime('%Y%m%d')}"):
        
        # Load and preprocess new data
        df_new = pd.read_csv(new_data_path)
        X, y = preprocess_for_training(df_new)
        
        # Split data
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )
        
        # Train model with best known hyperparameters
        model = XGBClassifier(**best_params)
        model.fit(X_train, y_train)
        
        # Evaluate
        val_metrics = evaluate_model(model, X_val, y_val)
        
        # Log metrics
        mlflow.log_metrics(val_metrics)
        
        # Compare with production model
        prod_metrics = get_production_model_metrics()
        
        if val_metrics['f1'] >= prod_metrics['f1'] * 0.95:  # Within 5%
            # Promote new model
            mlflow.sklearn.log_model(model, "model")
            logger.info(f"New model promoted: F1={val_metrics['f1']:.4f}")
            return True
        else:
            logger.warning(f"New model underperforms: F1={val_metrics['f1']:.4f}")
            return False
```

**Data Pipeline Automation**:
```python
# Example Airflow DAG structure
"""
uber_cancellation_pipeline:
  schedule: "0 2 * * 0"  # Weekly on Sunday at 2 AM
  
  tasks:
    - extract_new_data:
        description: Pull latest booking data
        
    - validate_data:
        description: Check data quality
        depends_on: extract_new_data
        
    - check_drift:
        description: Detect data/model drift
        depends_on: validate_data
        
    - retrain_model:
        description: Retrain if needed
        depends_on: check_drift
        trigger: drift_detected OR scheduled
        
    - evaluate_model:
        description: Validate new model
        depends_on: retrain_model
        
    - deploy_model:
        description: Deploy if improved
        depends_on: evaluate_model
        trigger: model_improved
        
    - notify:
        description: Send status notification
        depends_on: [deploy_model, evaluate_model]
"""
```

---

## Summary

This walkthrough has covered the complete machine learning project lifecycle for predicting Uber ride cancellations:

| Phase | Key Deliverables |
|-------|------------------|
| **1. Problem Framing** | Business objective, success metrics, assumptions |
| **2. Data Acquisition** | Kaggle dataset, 148K records, test set strategy |
| **3. Data Exploration** | EDA notebook, feature analysis, insights |
| **4. Data Preparation** | Preprocessing pipeline, feature engineering |
| **5. Model Selection** | Comparison of 6 models, short-list top 3 |
| **6. Fine-Tuning** | Hyperparameter optimization, ensemble methods |
| **7. Presentation** | Documentation, visualizations, business case |
| **8. Launch** | Production deployment, monitoring, retraining |

**Next Steps**:
1. Execute EDA notebook to validate hypotheses
2. Build and test preprocessing pipeline
3. Train and evaluate candidate models
4. Deploy MVP for A/B testing
5. Iterate based on production feedback

---

*Document Version: 1.0*  
*Last Updated: December 2024*  
*Author: Data Science Team*

